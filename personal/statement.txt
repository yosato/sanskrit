I am hereby applying for the position of Associate Senior Lecturer in Computational Linguistics.

As a candidate I am likely to be rather different from others, as I have been working, for the past four years, not in the academia but in the industry. I will also be different because the positions that I have filled since I finished my PhD are all very different from each other. In short, I have not followed a single-track academic path. I am confident however that this experience of mine gave me the wide-ranging experience and knowledge that few others can offer, and that such breadth is invaluable for interdisciplinary research conducted by your Centre.

The first of my past relevant positions was a theoretical one, although I was responsible for computational implementation. It concerned a specific semantico-syntactic framework, Dynamic Syntax, a semantics-based radical approach to grammar. My main contribution was implementing a Prolog-based parser faithful to the framework. I also contributed an article in a collection (Sato 2011) on the computational and cognitive implications drawn from this implementational work, which is to date my most voluminous work. It details of the graph-theoretic rendering of the parse-tree generation process, and I believe it gives solid computational underpinnings to the framework.

I was then employed for a very different project on 'developmental' robotics (European FP7 'ITALK' 2007-2012), that is about creating an agent that learns autonomously with data. A particular focus in this project was to simulate the acquisition of language by human infants in a data-driven way which however is also cognitively biased. We in fact used primarily the methods standardly used for supervised learning, though in a manner in which associations between various perceptions are assumed. For example we used k-nearest neighbour for word learning, by setting up experiments where humans talk to a robot while showing various shapes. Each set of visual features gets associated with a certain word in the experiment, thus allowing these features experienced by the robot subsequently trigger the utterance of the associated word. My main contribution then was, assuming these found words to be common nouns (in line with the common observation that children first acquire common nouns), to develop a grammar induction system whereby various formal semantic types are assigned to other parts of the sentence (Sato and Tam 2011). 

With the realisation that the above word-image association work presupposes the learning of words, I also engaged in the work on phonological word discovery. Sato et al (2013) is an attempt to simulate phonological word discovery in a realistic setting, combining prosody and dynamic time warping, by recovering repeated streams of sounds. I also summarise the 'chain' of learning scenarios, starting from phonoogical learning via word-image association to type-based grammar learning in Sato et al (2010).

While working in different areas, I always tried to bridge symbolic methods and probabilistic machine learning techniques. I am convinced that it is such balance that makes an effective learning possible, particularly the learning of the kind employed by human children. Using formal semantics in robotics is amongst a representative example. As applying probabilistic methods to type-theory is at the core of your group's research, I believe that I share the same principle with the Centre, and feel that I can make a concrete contribution to your research.

I then took up the current industry R&D position which involves the development of language models for handwriting recognition. The language models we develop are however very general, which can work for a variety of applications. Again while contributing some symbolic techniques such as finite state transducer to the purely probabilist colleagues, I learnt the fundamentals of language model building in a hands-on manner. I am familar therefore with how to deploy various models, from the classical N-gram model to more contemporary neural-net based models, as well as auxiliary techniques such as maximum entropy and distributional clustering. Furthermore, I have been working generally with raw training data and hence with unsupervised techniques, since we deal with many different languages and manual annotations are impractical. Working under this constraint has been challenging but has also led me to realise possibilities and limitations of unsupervised methods.

I launched and led various projects, most of which are to do with morphological processing of corpora. One of the first tasks was word segmentation of langugages with non-spaced orthography such as Chinese and Japanese, and while using ready-trained CRF models, I augmented the model's capability to detect out-of-vocabulary words using maximum entropy modelling. I also extended N-gram models to deal with variable unit lengths by incorporating a mutual-information based mechanism to detect multi-word expressions. I used the same MI technique to find unknown words in Chinese as well. For Japanese and Korean, I deployed chunk-based language models, which were rule-based. Thus different tasks required different solutions, but all generally concerns the granularity of linguistic units, which now constitutes one of my main theoretical interests. 

Another line of work I engage in is dialect processing, or 'adaptation' of the standard model to dialects and their classification. I treat the adaptation as a form of incremental semi-supervised learning, where the standard model adapts itself when faced with the 'consistently noisy' input. The original framework comes from the 'cognate pair detection' work of Scherrer and Sagot (2013) who work on already classified datasets, but I extended it so that the classification can also be done automatically (Sato, submitted), using the idea of distance between sentences relative to a language model (Kita 1999). I also used the popular skip N-gram word embedding tool ('word2vec') to compute the context similarity to identify cognate pairs.

A general observation through carrying out these tasks is that unsupervised learning goes quite far, and I come to believe that such domain-general statistical learning plays an important part in humans' acquisition of 'word-like units'. The work I have been doing at the current position however only tangentially touches upon the most important aspect of the language, meaning. I would very much embark on the semantic work your group is focusing on, particularly now that semantics-oriented statistical work is burgeoning in the mainstream of NLP with the advent of word-embeddings. It is an exciting prospect to apply these newly developed techniques to type-theoretic semantics.

I would like to end this statement with a change I want to happen and make happen in computer science: more collaboration and movement of resources, human or financial, between academia and industry. In the industry I have seen many talented people who stop publishing and developing novel ideas either being too busy coding or due to confidentiality. In the academia on the other hand many people are overwhelmed by administration, teaching and paper deadlines, leaving no time for serious implementation. I see no other way than the two working together more collaboratively to avoid such waste. Having been in both environments and seen both problems and merits in each, I would dearly like to promote an open research project in which the industry can participate.

References

Sato et al. 2013		'I think I have heard that one before: Recurrence-based word learning with a robot' (with Ze Ji and Sander van Dijk), in Gogate L. ed, Theoretical and Computational Models of Word Learning: Implications for Psychologyand Artificial Intelligence, IGI Global, 2013.

Sato and Tam 2011		Underspecified types and the semantic bootstrapping of common nouns and adjectives: a simulation with a robot’s sensory data (with Tam Wailok), the Logic and Engineering of Natural Language Semantics (LENLS8), 2011.

Sato 2011    		‘Local Ambiguity, Search Strategies and Parsing in Dynamic Syntax’ in Kempson et al (eds) The Dynamics of Lexical Interfaces, CSLI, 2011.

Sato et al. 2010		An Integrated Three-Stage Model towards Grammar Acquisition (co-authored), the International Conference on Development and Learning (ICDL), 2010

Sato, submitted			Semi-supervised classification of dialects from a mixed corpus. 

Kita 1999			 Automatic clustering of languages based on probabilistic models. Journal of Quantitative Linguistics

Scherrer and Sagot 2013		Lexicon induction and part-of-speech tagging of non-resourced languages without any bilingual resources,  RANLP Workshop on Adaptation of language resources and tools for closely related languages and language variant

\documentclass{article}
\usepackage{acl2016}

\begin{document}

\title{Is a multi-word expression possible without words? \\A syllable-based model}

\maketitle

\section{Motivation}

It would most probably sound just tautological to state that multi-word expressions are about sequences of \emph{words}. However, `word' is not so safe a concept: linguists have debated, without much clear conclusion, what a word is for a long time. One contentious issue concerns \emph{granularity}. Some `multi-word' expressions are ---say `hand writing' in English or `petit dejeuner' in French--- may just be pereived as single words, as the fact that they are also written without space indicates. On the other hand, some semantically complex expressions --- --- could be perceived as multiple words.  or sub-lexical units (morphemes) depending on who you ask. Thus a `multi-word expression' can consist of two words according to some, and three for others, and after all, of one, which would not be multi-word at all. So the very building block of multi-word expression is rather fragile. 

Also, the `words', or their boundaries, should not be just taken for granted if any study were to construct a \emph{learning} model, which this work aspires to represent. Word boundaries are simply absent in continuous speech, from which children acquire the command of the language, including the vocabulary. In fact, there is evidence that the infant learners start out `wholistically', that is learn indeed `multi-word' expressions apparently without much internal analysis \cite{}. What is interesting here is they generally do respect word boundaries, although they clearly have not learnt so many `words', if at all. This phenomenon thus indicates there is some sense in which multi-word expressions are learnt based on sub-lexical units.

In light of this we will examine in what follows a model based on \emph{syllables} and how it compare it with what is regarded as one based on words, that is, in the normal sense of the term. Certainly the model based on syllables cannot per se provide that of words, since it would generate non-words at its peripheries. It cannot, in principle, outperform the word-model, as long as the word is correctly segmented. We nevertheless believe that this study will contribute to the research of multi-word expressions, in addition to the potential as a learning model, in at least two ways. One is to provide a reference for the word-based model. The syllable model would not only give the upper bound of the counterpart word model, but can shed light in terms of granularity on the word model, which can be oversegmenting or undersegmenting to varying degrees. Another is the usefulness to the languages which do not employ word-based orthgraphy such as Chinese. If we were to build a model for multi-word expressions on words in such a language, 

A possible Hypothesis is

Questions addressed are...

\section{Target languages and corpora}

A pre-requisite of syllable-based model is, just as `spaceless' languages require segmentation into words, that the input be segmented into syllables. Syllabification could however be a complex task, 

Vietnamese, Japanese and Chinese.

Weibo and 

\section{Methods}

There are some well-known methods to extract multi-word expressions \cite{}. One of the most used would be an information-theoretic measure, pointwise mutual information (PMI). 

$$ PMI=\frac{log_2 P(w_1,W_2)}{log_2 P(w_1)P(w_2)} $$

PMI however favours infrequent words, which is not so desirable for the purpose of 

Mutual information concerns two units (random variables), or bigrams in our context. Therefore in order to apply it to N-grams in general, as has been done  , we start with a bigram and extend it to N+1-grams by taking N-grams as the first item of a `bigram' (generalised bigram) to compute the mutual information of N-grams in general for any N.




As one can see, one of the practical, and to a degree cognition-related, problem is the complexity of counting such generalised bigrams. The possible combinations of two units is already exponential ($a$ in general where $n$ is the size of the vocabulary) in general case, but it is rather the fact that in each iteration the resulting N+1-grams extracted in an iteration add to the vocabulary in the next iteration that makes the whole procedure genuinely intractable in practice. 

In order to keep the computation tractable,  employed in \cite{} , to stop  N+1-gram iteration, as follows

The intuition behind this is 

 





\section{Experiment}

We have conducted on Vietnamese



\section{Evaluation}

As we have mentioned, the main focus of our study is whether a sub-lexical level model for multi-word expressions could work as well as a word-level one. Therefore we take the word-level results to be the reference, or upper bound, and see how close a result the syllable-models achieves.

However, it is also of interest, as mentioned briefly in the introduction, whether a syllable-based model is there are cases where the word-based model can get rectified by the sub-lexical model. perplexity.

\section{Results}

The results are shown in Table 


\section{Future work}

more children

\bibliography{yobib}
\bibliographystyle{acl2016}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

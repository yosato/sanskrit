\documentclass[11pt]{article}
\usepackage{coling08}
\usepackage{times}
\usepackage{latexsym}
%\setlength\titlebox{6.5cm}    % Expanding the titlebox

\pagestyle{empty}

%\usepackage[sectionbib]{natbib}

%\usepackage{avm+}
%\usepackage{myMacros}
\usepackage{times}
\usepackage{latexsym}
\usepackage{relsize}
\usepackage{epic}
%ecltree++}
\usepackage{lingmacros}
\usepackage{listings}
\usepackage[dvips]{color}
%\usepackage{soul}
%\usepackage{hyperref}
%\usepackage{multirow}
\usepackage{rotating}
\usepackage{amsmath}

%\setlength{\textwidth}{480pt}
%\setlength\topmargin{0mm}
%\setlength\oddsidemargin{-3mm}
%\setlength\evensidemargin{-3mm}

%% Listings
\lstset{%
  language=Python,%
  basicstyle=\small,%
  keywordstyle=\color{blue},%
  commentstyle=\color{red},%
  stringstyle=\color{magenta},%
  frame=lines,%
  % backgroundcolor=\color{linen},
  % blankstring=true,%
  showstringspaces=false,%
  extendedchars,%
  % fancyvrb,
  morekeywords={tok,cons,sentence},%
  % lineskip=-4pt,
  fontadjust,%
  breaklines,%
  numbers=left,%
  stepnumber=2,%
  numbersep=5pt,%
  numberstyle=\small%
}

\def\<{$\langle$}
\def\>{$\rangle$}




\title{Parser Evaluation across Frameworks without Format Conversion}
\author{\parbox[t]{5cm}{\centering Wai Lok Tam\\{\rm Interfaculty Initiative in\\Information Studies\\University of Tokyo\\7-3-1 Hongo Bunkyo-ku\\ Tokyo
113-0033 Japan}}\parbox[t]{5cm}{\centering Yo Sato\\{\rm Dept of Computer Science\\Queen Mary\\University of London\\Mile End Road\\London E1 4NS, U.K.}}\parbox[t]{5cm}{\mbox{Yusuke Miyao \hspace{1em} Jun'ichi Tsujii}\\\parbox{5cm}{\centering {\rm \vspace{1mm}Dept of Computer Science\\University of Tokyo\\7-3-1 Hongo Bunkyo-ku\\Tokyo113-0033 Japan } }} }

%\hspace{3mm}Yusuke Miyao\\\hspace{3.3cm}Dept of Computer Science\\\hspace{3.3cm}University of Tokyo\\\hspace{3.3cm}7-3-1 Hongo Bunkyo-ku\\\hspace{3.3cm}Tokyo113-0033 Japan \And \hspace{-12mm}Jun-ichi Tsujii}

\date{}

\begin{document}
\maketitle

\begin{abstract}
 
\end{abstract}



\section{Introduction}

%Although a computational grammar is usually part of a larger system, the name of the field, grammar engineering suggests that it is central to the system, at least from the point of views of grammar writers. This keeps grammar writers to focus on what they can do with a grammar when they want to look for a solution to a problem. To illustrate, grammar writers tend to rely more on inline comments than a separate manual for documenting a grammar. Such solution may not always be the best solution for a problem. If we are looking for documentation for grammar writers who have the need to understand how a grammar works and to edit the grammar, inline comments make a good solution.  But if we are looking for documentation for users, inline comments make very little sense to them, if they can access these comments at all. In this paper, we describe a solution for a different problem, the evaluation of a grammar. Our solution to this problem lies not in our grammars but in the manuals in which we provide the correct output for the linguistic phenomena covered by our two grammars. The idea is to match the output of a grammar against the sample output given in its manual. This means each grammar would get its own measuring stick. This is a major difference between our approach to evaluation and many other evaluation methods which make use of one measuring stick.

The traditional evaluation method for a deep parser is to test it against a list of sentences, each of which is paired with a yes or no. The parser is evaluated on the number of  grammatical sentences it accepts and that of ungrammatical sentences it rules out. A problem with this approach to evaluation is that it neither penalizes a parser for getting an analysis wrong for a sentence nor rewards it for getting it right. What prevents the NLP community from working out a universally applicable reward and penalty scheme is the absence of a gold standard that can be used across frameworks. The correctness of an analysis produced by a parser  can only be judged by matching it to the analysis produced by linguists in syntactic structures and semantic representations created specifically for the framework on which the grammar is based. A match or a mismatch between analyses produced by different parsers based on different frameworks does not lend itself for a meaningful comparison that leads to a fair evaluation of the parsers. To evaluate two parsers across frameworks, two kinds of methods suggest themselves:

\begin{enumerate}
\item Converting an analysis given in a certain format native to one framework to another native to a differernt framework (e.g. converting from a CCG \cite{Steedman:00} derivation tree to an HPSG \cite{pollardandsag1994} phrase structure tree with AVM)
\item Converting analyses given in different framework-specific formats to some simpler format proposed as a framework-independent evaluation schema  (e.g. converting from HPSG phrase structure tree with AVM to GR \cite{briscodandcarrollandwatson2006})
\end{enumerate}

However, the feasibility of either solution is questionable. Even conversion between two evaluation schemata which make use of the simplest representation of syntactic information in the form of dependencies is reported to be problematic by \cite{miyaoandsagaeandtsujii2007}. 

In this paper, therefore, we propose a different method of parser evaluation that makes no attempt at any conversion of syntactic structures and semantic representations. We remove the need for such conversion by abstracting away from  comparison of syntactic structures and semantic representations. The basic idea is to generate a list of names of phenomena with each parse. These names of phenomena are matched against the phenomena given in the gold standard for the same sentence. The number of matches found is used for evaluating the parser that produces the parse.

\section{Research Problem}

Grammar formalisms differ in many aspects. In syntax, they differ in POS label assignment, phrase structure (if any), syntactic head assignment (if any) and so on, while in semantics, they differ from each other in semantic head assignment, role assignment, number of arguments taken by predicates, etc. Finding a common denominator between grammar formalisms in full and complex representation of syntactic information and semantic information has been generally considered by the NLP community to be an unrealistic task, although some serious attempts have been made recently to offer simpler representation of syntactic information \cite{briscodandcarrollandwatson2006,demarneffeandmaccartneyandmanning2006}. 
 
Briscoe et al \shortcite{briscodandcarrollandwatson2006}'s Grammatical Relation (GR) scheme is proposed as a framework-independent metric for parsing accuracy. The promise of GR lies actually in its dependence on a framework that makes use of simple representation of syntactic information. The assumption behind the usefulness of GR for evaluating the output of parsers %based on a grammar formalism other than Dependency Grammar 
is that most conflicts between grammar formalisms would be removed by discarding less useful information carried by complex syntactic or semantic representations used in grammar formalisms during conversion to GRs. But is this assumption true? The answer is not clear. A GR represents syntactic information in the form of  a binary relation between a token assigned as the head of the relation and other tokens assigned as its dependents. Notice however that grammar frameworks considerably disagree in the way they assign heads and non-heads. This would raise the doubt that, no matter how much information is removed, there could still remain disagreements between grammar formalisms in what is left.

The simplicity of GR, or other dependency-based metrics, may give the impression that conversion from a more complex representation into it is easier than conversion between two complex representations. In other words, GRs or a similar dependency relation looks like a promising candidate for \emph{lingua franca} of grammar frameworks. However the experiment results given by  Miyao et al \shortcite{miyaoandsagaeandtsujii2007} show that even conversion into GRs of predicate-argument structures, which is not much more complex than GRs, is not a trivial task.  Miyao et al \shortcite{miyaoandsagaeandtsujii2007} manage to convert 80\% of the predicate-argument structures outputted by their deep parser, ENJU, to GRs correctly. However the parser, with an over 90\% accuracy, is too good for the 80\% conversion rate. The lesson here is that simplicity of a representation is a different thing from simplicity in converting into that representation. 


\section{Outline of our Solution}

The problem of finding a  common denominator for grammar formalisms and the problem of conversion to a common denominator may be best addressed by evaluating parsers without making any attempt to find a common denominator or conduct any conversion. Let us describe briefly in this section how such evaluation can be realised.  


\subsection{Creating the Gold Standard} 
The first step of our evaluation method is to construct or find a number of sentences and get an annotator to mark each sentence for the phenomena illustrated by each sentence. After annotating all the sentences in a test suite, we get a list of pairs, whose first element is a sentence ID and second is again a list, one of the corresponding phenomena. This list of pairs is our gold standard. To illustrate, suppose we only get sentence \ref{johngivesaflowertomary} and sentence \ref{johngivesmaryaflower} in our test suite.

\enumsentence{John gives a flower to Mary}\label{johngivesaflowertomary}



\enumsentence{John gives Mary a flower}\label{johngivesmaryaflower}

Sentence \ref{johngivesaflowertomary} is assigned the phenomena: proper noun, unshifted ditransitive, preposition. Sentence \ref{johngivesmaryaflower} is assigned the phenomena: proper noun, dative-shifted ditransitive. Our gold standard is thus the following list of pairs: 

\smallskip

\< \<\ref{johngivesaflowertomary}, $\langle{}${\smaller proper noun, unshifted ditransitive, preposition}\>,

\hspace{1.5mm} \<\ref{johngivesmaryaflower}, $\langle{}${\smaller proper noun,dative-shifted ditransitive}$\rangle$ 
$\rangle$



\subsection{Phenomena Recognition}


The second step of our evaluation method requires a small program that recognises what phenomena are illustrated by an input sentence taken from the test suite based on the output resulted from parsing the sentence. The recogniser provides a set of conditions that assign names of phenomena to an output, based on which the output is matched with some framework-specific regular expressions. It looks for hints like the rule being applied at a node, the POS label being assigned to a node, the phrase structure and the role assigned to a reference marker. The names of phenomena assigned to a sentence are stored in a list. The list of phenomena forms a pair with the ID of the sentence, and running the recogniser on multiple outputs obtained by batch parsing (with the parser to be evaluated) will produce a list of such pairs, in exactly the same format as our gold standard. Let us illustrate this with a parser that:


\begin{enumerate}
\item assigns a monotransitive verb analysis to `give' and an adjunct analysis to `to Mary' in \ref{johngivesaflowertomary}

\item assigns a ditransitive verb analysis to `give' in  \ref{johngivesmaryaflower} 
\end{enumerate}


The list of pairs we obtain from running the recogniser on the results produced by batch parsing the test suite with the parser to be evaluated is the following:

\smallskip

\<\<\ref{johngivesaflowertomary},\<{\smaller proper noun,monotransitive,preposition,adjunct}\>\>, 

\hspace{1.5mm}\<\ref{johngivesmaryaflower}, \<{\smaller proper noun,dative-shifted ditransitive}\> \>

\subsection{Performance Measure Calculation}

Comparing the two list of pairs generated from the previous steps, we can calculate the precision and recall of a parser using the following formulae:
\begin{align}
          Precision &= (\sum_{i=1}^n \frac{\mid\{R_{i}\}\cap\{A_{i}\}}{\mid\{R_{i}\}\mid})\div n
\end{align}

\begin{align}
          Recall &= (\sum_{i=1}^n \frac{\mid\{R_{i}\}\cap\{A_{i}\}}{\mid\{A_{i}\}\mid})\div n
\end{align}

\noindent where list $R_i$  is the list generated by the recogniser for sentence $i$, list $A_{i}$ is the list produced by annotators for sentence $i$, and $n$ the number of sentences in the test suite.

In our example, the parser that does a good job with dative-shifted ditransitives but does a poor job with unshifted ditranstives would have a precision of:

 \begin{align*}
            (\frac{2}{4} + \frac{2}{2})\div 2 &= 0.75
\end{align*}

\noindent and a recall of:

 \begin{align*}
            (\frac{2}{3} + \frac{2}{2})\div 2 &= 0.83
\end{align*}


\section{Refining our Solution}

\subsection{Exhaustive Annotation vs Non-exhaustive Annotation}
In order for the precision and recall given above to be a fair measure, it is necessary for both the recogniser and the annotators to produce an exhaustive list of the phenomena illustrated by a sentence. % 
%
%\begin{lstlisting}
%def recognise_or_annotate(NODE)
%    global(PHENOMENON_LIST,FLAG_LIST) 
%    FLAG_LIST.append(generate_satisfied_requirements_for\\
%_phenomena_that_involves_>1_constituents(NODE))
%    PHENOMENON_LIST.append(translate_pos_label_to_name_of\\
%_phenomena(NODE))%

%def expand_tree(MOTHER):
%    DAUGHTER_LIST = generate_daughters(MOTHER)
%    if len(DAUGHTER_LIST) = 1:
%        recognise_or_annotate(DAUGHTER_LIST[0])
%    else:
%#assume that non-binary tree is not allowed
%        recognise_or_annotate(DAUGHTER_LIST[0])
%        expand_tree(DAUGHTER_LIST[0])
%        recognise_or_annotate(DAUGHTER_LIST[1])
%        expand_tree(DAUGHTER_LIST[1])

%def find_phenomena_whose_requirements_r_\\
%satisfied(FLAG_LIST):
%    global(PHENOMENON_LIST)
%    for PHENOMENON in PREDEF_LIST_OF_PHENOMENA_THAT_INVOLVE_>1_CONSTITUENTS:
%        REQ_LIST = generate_requirements_for_phenomena_that_involves_>1_constituents(PHENOMENON)
%        for REQ in REQ_LIST:
%            SATISFIED = 0
%            for FLAG in FLAG_LIST:
%                if FLAG==REQ:
%                    SATISFIED=1
%        if SATISFIED == 1:
%            PHENOMENON_LIST.append(PHENOMENON)
%
%#begin main program
%while 1:
%    SENTENCE=TESTSUITE.readline()
%    if not SENTENCE: break
%    expand_tree(SENTENCE)
%    find_phenomena_whose_requirements_r_satisfied(FLAG_LIST)
    
%\end{lstlisting}

%Our recogniser(or a human annotator) is supposed to go through each node in a binary phrase structure tree from top to bottom until all the leafs are processed. The processing of each node is done by the procedure \textit{recognise\_or\_annotate}. This procedure calls two procedures: \textit{generate\_satisfied\_requirements\_for\_phenomena\_that\_involves\_>1\_constituents} and \textit{translate\_pos\_label\_to\_name\_of\_phenomena}. The names fo these procedures tell what they do. \textit{generate\_satisfied\_requirements\_for\_phenomena\_that\_involves\_>1\_constituents}  deals with phenomena that involves more than one constituents. An example of such phenomena is ''nominal gerund''. A possessive and a gerund that takes a noun phrase complement are required for forming a nominal gerund. Given a possessive, this procedure would mark one of the requirements for forming a nominal gerund as satisfied with a flag to be appended to a list of flags. The other procedure \textit{translate\_pos\_label\_to\_name\_of\_phenomena} deals with phenomena that involves only one constituent. The name of one such phenomenon is just what the POS label of a node stands for. For example, the POS label NNP would be translated into the phenomenon ''proper noun''. After processing all the leafs, we get a list of all the phenomena that involves only one consitutent and a list of flags that tell, among the phenomena that involve more than one constituents, which requirements for which of these phenomena are satisfied. The last thing that our recogniser (or a human annotator) has to do is to call \textit{find\_phenomena\_whose\_requirements\_r\_satisfied}. This procedure does what its name says, that is, finding out,  among the phenomena that involve more than one constituents, those phenomena whose requirements are all satisfied and append them to the list of phenomena that involves only one constituent. 
%
But we foresee that annotation errors are likely to be a problem of exhaustive annotation, as is reported in Miyao et al \shortcite{miyaoandsagaeandtsujii2007} for the gold standard described in Briscoe et al \shortcite{briscodandcarrollandwatson2006}. Exhaustive annotation procedures require annotators to repeatedly parse a sentence in search for a number of phenomena, which is not the way language is normally processed by humans. Forcing annotators to do this, particularly for a long and complex sentence, is a probable reason for the annotation errors in the gold standard described in \cite{briscodandcarrollandwatson2006}.

To avoid the same problem in our creation of a gold standard, we propose to allow non-exhaustive annotation. In fact, our proposal is to limit the number of phenomena assigned to a sentence to one. This decision on which phenomenon to be assigned is made, when the test suite is constructed, for each of the sentences contained in it. Following the traditional approach, we include every sentence in the test suite, along with the core phenomenon we intend to test it on \cite{lehmannandoepen1996}. %During the construction of the gold standard, the phenomenon assigned to every sentence is the core phenomenon we want to test with the sentence when we include it in the test suite. 
Thus, Sentence \ref{johngivesaflowertomary} would be assigned the phenomenon of unshifted ditransitive. Sentence \ref{johngivesmaryaflower} would be assigned the phenomenon of dative-shifted ditransitive. This revision of annotation policy removes the need for exhaustive annotation. Instead, annotators are given a new task. They are asked to assign to each sentence \emph{the most common error that a parser is likely to make}.  Thus Sentence \ref{johngivesaflowertomary} would be assigned adjunct for such an error. Sentence \ref{johngivesmaryaflower} would be assigned the error of noun-noun compound. Note that these errors are also names of phenomena. 

This change in annotation policy calls for a change in the calculation of precision and recall. We leave the recogniser as it is, i.e. to produce an exhaustive list of phenomena, since it is far beyond our remit to render it intelligent enough to select a single, intended, phenomenon. Therefore, an incorrectly low precision would result from a mismatch between the exhaustive list generated by the recogniser and the singleton list produced by annotators for a sentence. %$\mid\{phenomena\ in\ list\ R_{i}\}\cap\{phenomena\ in\ list\ A_{i}\}\mid$ would be 1 no matter how many phenomena illustrated by a sentence but not assigned to the sentence in the gold standard is generated by the recogniser. 
For example, suppose we only have sentence \ref{johngivesmaryaflower} in our test suite and the parser correctly analyses the sentence. Our recogniser assigns two phenomena (proper noun, dative-shifted ditransitive) to this sentence as before. This would result in a precision of 0.5. 

Thus we need to revise our definition of precision, but before we give our new definition, let us define a truth function $t$:

\[
t(A \supset B) = \left\{ 
\begin{array}{ccc}
1 & A & \supset B  \\
0 & A \cap B & =\emptyset 
\end{array} \right.
\]

\[
t(A \cap B=\emptyset) = \left\{ 
\begin{array}{ccc}
0 & A \cap B & \neq \emptyset \\
1 & A \cap B & =\emptyset 
\end{array} \right.
\]

Now, our new definition of precision and recall is as follows:

\begin{align}
        &  Precision  \\
       =& \frac{(\sum_{i=1}^n \frac{t(\{R_{i}\}\supset\{AP_{i}\})+t(\{R_{i}\}\cap\{AN_{i}\}=\emptyset)}{2})}{n} \notag
\end{align}\label{redefinedprecision}

\begin{align}
        & Recall \\
       =& \frac{(\sum_{i=1}^n \frac{\mid\{R_{i}\}\cap\{AP_{i}\}}{\mid\{AP_{i}\}\mid})}{n} \notag
\end{align}


\noindent where list $AP_{i}$ is the list of phenomena produced by annotators for sentence $i$, and list $AN_{i}$ is the list of errors produced by annotators for sentence $i$.

While the change in the definition of recall is trivial, the new definition of precision requires some explanation. The exhaustive list of phenomena generated by our recogniser for each sentence is taken as a combination of two answers to two questions on the two lists produced by annotators for each sentence. The correct answer to the question on the one-item-list of phenomenon produced by annotators for a sentence is a superset-subset relation between the list generated by our recogniser and the one-item-list of phenomenon produced by annotators. The correct answer to the question on the one-item-list of error produced by annotators for a sentence is the non-existence of any common member between the list generated by our recogniser and the one-item-list of error produced by annotators.

To illustrate, let us try a parser that does a good job with dative-shifted ditransitives but does a poor job with unshifted ditranstives on both  \ref{johngivesmaryaflower} and \ref{johngivesaflowertomary}. The precision of such a parser would be:

 \begin{align*}
            (\frac{0}{2} + \frac{2}{2})\div 2 &= 0.5
\end{align*}

and its recall would be:

\begin{align*}
            (\frac{0}{1} + \frac{1}{1})\div 2 &= 0.5
\end{align*}

\section{Experiment}

For this abstract, we evaluate ENJU \cite{miyao2006}, a released deep parser based on the HPSG formalism and a parser based on the Dynamic Syntax formalism \cite{kempsonandmeyerviolandgabbay2001} under development against the gold standard given in table \ref{experiment}.
%\begin{figure}
\begin{figure}
\begin{tabular}{| c | p{80pt} | p{80pt} |}
\hline
ID & Phenomenon & Error \\
\hline
\hline
1 &   unshifted ditransitive & adjunct \\
2 &   dative-shifted ditransitive & noun-noun compound \\
3 &   passive & adjunct \\
4 &  nominal gerund & verb that takes verbal complement \\
5 &  verbal gerund & imperative\\
6 &  preposition & particle \\
7 &  particle & preposition\\
8 & adjective with extrapolated sentential complement & relative clause\\
9 &  inversion & question \\
10 &  raising & control \\ 
\hline
\end{tabular}
\caption{Gold Standard for Parser Evaluation}\label{experiment}
\end{figure}
%\caption{}
%\end{figure}

\begin{figure}
\begin{tabular}{| c | c |}
\hline
ID & Sentence \\
\hline
\hline
1 &  John gives a flower to Mary \\
2 &  John give Mary a flower \\
3 &  John is dumped by Mary \\
4 & Your walking me pleases me \\
5 &  Abandoning children increased\\
6 & He talks to Mary \\
7 &  John makes up the story \\
8 & It is obvious that John is a fool \\
9 &  Hardly does anyone know Mary\\
10 &  John continues to please Mary \\ 
\hline
\end{tabular}
\caption{Sentences Used in the Gold Standard}\label{sentences}
\end{figure}

The precision and recall of the two parsers (ENJU and DSPD, which stands for ''Dynamic Syntax Parser under Development'')  are given in table \ref{result}:

\begin{figure}
\begin{tabular}{| c | c  | c |}
\hline
  Measure & ENJU & DSPD \\
\hline
  Precision & 0.8 & 0.7 \\
  Recall & 0.7 & 0.5 \\ 
\hline
\end{tabular}
\caption{Performance of Two Parsers}\label{result}
\end{figure}

The experiment that we report here is intended to be an experiment with the evaluation method described in the last section, rather than a very serious attempt to evaluate the two parsers in question. The sentences in table \ref{experiment} are carefully selected to include both sentences that illustrate core phenomena and sentences that illustrate rarer but more interesting (to linguists) phenomena. But there are too few of them. In fact, the most important number that we have obtained from our experiment is the 100\% success rate in recognizing the phenomena given in table \ref{experiment}.

\section{Discussion}

\subsection{Recognition Rate}
The 100\% success rate is not as surprising as it may look. We made use of two recognisers, one for each parser. Each of them is written by the one of us who is somehow involved in the development of the parser whose output is being recognised and familiar with the formalism on which the output is based. This is a clear advantage to format conversion used in other evaluation methods, which is usually done by someone familiar with either the source or the target of conversion, but not both, as such a recogniser only requires knowledge of one formalism and one parser. For someone who is involved in the development of the grammar and of the parser that runs it, it is straightforward to write a recogniser that can make use of the code built into the parser or rules included in the grammar. We can imagine that the 100\% recognition rate would drop a little if we needed to recognise a large number of sentences but were not allowed sufficient time to write detailed regular expressions. Even in such a situation, we are confident that the success rate of recognition would be higher than the conversion method. 

Note that the effectiveness of our evaluation method depends on the success rate of recognition to the same extent that the conversion method employed in Briscoe et al. \shortcite{briscodandcarrollandwatson2006} and de Marneff et al. \shortcite{demarneffeandmaccartneyandmanning2006} depends on the conversion rate. Given the high success rate of recognition, we argue that our evaluation method is more effective than any evaluation method which makes use of a format claimed to be framework independent and involves conversion of output based on a different formalism to the proposed format. 

\subsection{Strictness of Recognition and Precision}

There are some precautions regarding the use of our evaluation method. The redefined precision \ref{redefinedprecision} is affected by the strictness of the recogniser. To illustrate, let us take Sentence 8 in Table \ref{experiment} as an example. ENJU provides the correct phrase structure analysis using the desired rules for this sentence but makes some mistakes in assigning roles to the adjective and the copular verb. The recogniser we write for ENJU is very strict and refuses to assign the phenomenon `adjective with extrapolated sentential complement' based on the output given by ENJU. So ENJU gets 0 point for its answer to the question on the singleton list of phenomenon in the gold standard. But it gets 1 point for its answer to  the question on the singleton list of error in the gold standard because it does not go to the other extreme: a relative clause analysis, yielding a 0.5 precision. In this case, this value is fair for ENJU, which produces a partially correct analysis. However, a parser that does not accept the sentence at all, a parser that fails to produce any output or one that erroneously produces an unexpected phenomenon would get the same result: for Sentence 8, such a parser would still get a precision of 0.5, simply because its output does not show that it assigns a relative clause analysis. 

We can however rectify this situation. For the lack of parse output, we can add an exception clause to make the parser automatically get a 0 precision (for that sentence). Parsers that make unexpected mistakes are more problematic. An obvious solution to deal with these parsers is to come up with an exhaustive list of mistakes but this is an unrealistic task. For the moment, a temporary but realistic solution would be to expand the list of errors assigned to each sentence in the gold standard and ask annotators to make more intelligent guess of the mistakes that can be made by parsers by considering factors such as similarities in phrase structures or the sharing of sub-trees. 

\subsection{Combining Evaluation Methods}

For all measures, some distortion is unavoidable when applied to exceptional cases. This is true for the classical precision and recall, and our redefined precision and recall is no exception. In the case of the classical precision and recall, the distortion is countered by the inverse relation between them so that even if one is distorted, we can tell from the other that how well (poorly) the object of evaluation performs. Our redefined precision and recall works pretty much the same way. 

What motivates us to derive measures so closely related to the classical precision and recall is the ease to combine the redefined precision and recall obtained from our evaluation method with the classical precision and recall obtained from other evaluation methods, so as to obtain a full picture of the performance of the object of evaluation. For example, our redefined precision and recall figures given in Table \ref{result} (or figures obtained from running the same experiment on a larger test set)  for ENJU can be combined with the precision and recall figures given in Miyao et al. \shortcite{miyao2006} for ENJU, which is based on a evaluation method that compares its predicate-argument structures those given in Penn Treebank. Here the precision and recall figures are calculated by assigning an equal weight to every sentence in Section 23 of Penn Treebank. This means that different weights are assigned to different phenomena depending on their frequency in the Penn Treebank. Such assignment of weights may not be what linguists or developers of NLP systems want who are targeting a corpus with a very different distribution of phenomena from this particular section of the Penn Treebank. For example, a linguist may wish to assign an equal weight across phenomena or more weights to `interesting' phenomena. A developer of a question-answering system may wish to give more weights to question-related phenomena than other phenomena of less interest which are nevertheless attested more frequently in the Penn Treebank. 

In sum, the classical precision and recall figures calculated by assigning equal weight to every sentence could be considered skewed from the perspective of phenomena, whereas our redefined precision and recall figures may be seen as skewed from the frequency perspective. Frequency is relative to domains: less common phenomena in some domains could occur more often in others. Our redefined precision and recall are not only useful for those who want a performance measure skewed the way they want, but also useful for those who want a performance measure as `unskewed' as possible. This may be obtained by combining our redefined precision and recall with the classical precision and recall yielded from other evaluation methods.


\section{Conclusion}  



%The problem of finding a  common denominator for grammar formalisms and the problem of conversion to a common denominator may be best addressed by not finding a common denominator and not doing any conversion. For each grammar we intend to evaluate, we create a separate set of sample output in a format native to the grammar or a simplified format close-to-native to the grammar. The representations given as sample output are sorted by linguistic phenomena. Then we create two small programs, one for recognizing the linguistic phenomenon illustrated by an input sentence in a test-suite, which includes a equal number of sentences for each phenomenon covered by the grammar, another for  matching the output obtained from parsing the input sentence to the sample output given for the recognised phenomenon in the manual created for that grammar. These two programs are just sets of regular expressions. Evaluation of two grammars can be done by comparing the precision of the pasers loaded with them and the coverage of them. The precision of a parser loaded with a grammar for evaluation can be obtained by:



 


%\section{Problems with One Measuring Stick for All Formalism}

%Using one measuring stick for evaluation makes much more sense than using a different measuring stick for every grammar to be evaluated. But there are two difficult problems with using a single measuring stick for evaluating more than one grammars. 




%For each grammar we intend to evaluate, we create a separate set of sample output in a format native to the grammar or a simplified format close-to-native to the grammar. The representations given as sample output are sorted by linguistic phenomena. Then we create two small programs, one for recognizing the linguistic phenomenon illustrated by an input sentence in a test-suite, which includes a equal number of sentences for each phenomenon covered by the grammar, another for  matching the output obtained from parsing the input sentence to the sample output given for the recognised phenomenon in the manual created for that grammar. These two programs are just sets of regular expressions.

%\subsection{Implementation}

%We have tried this evaluation method with the grammar of ENJU \cite{miyao2005}, a released deep parser based on HPSG \cite{pollardandsag1994} and a grammar under development based on Dynamic Syntax \cite{kempsonandgabbayandmeyerviol2001}.  
\bibliographystyle{coling}

\bibliography{yobib}



\end{document}

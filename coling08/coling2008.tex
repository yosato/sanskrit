\documentclass[11pt]{article}
\usepackage{coling08}
\usepackage{times}
\usepackage{latexsym}
%\setlength\titlebox{6.5cm}    % Expanding the titlebox

\pagestyle{empty}

%\usepackage[sectionbib]{natbib}

%\usepackage{avm+}
%\usepackage{myMacros}
\usepackage{times}
\usepackage{latexsym}
\usepackage{relsize}
\usepackage{epic}
%ecltree++}
\usepackage{lingmacros}
\usepackage{listings}
\usepackage[dvips]{color}
%\usepackage{soul}
%\usepackage{hyperref}
%\usepackage{multirow}
\usepackage{rotating}
\usepackage{amsmath}

%\setlength{\textwidth}{480pt}
%\setlength\topmargin{0mm}
%\setlength\oddsidemargin{-3mm}
%\setlength\evensidemargin{-3mm}

%% Listings
\lstset{%
  language=Python,%
  basicstyle=\small,%
  keywordstyle=\color{blue},%
  commentstyle=\color{red},%
  stringstyle=\color{magenta},%
  frame=lines,%
  % backgroundcolor=\color{linen},
  % blankstring=true,%
  showstringspaces=false,%
  extendedchars,%
  % fancyvrb,
  morekeywords={tok,cons,sentence},%
  % lineskip=-4pt,
  fontadjust,%
  breaklines,%
  numbers=left,%
  stepnumber=2,%
  numbersep=5pt,%
  numberstyle=\small%
}

\def\<{$\langle$}
\def\>{$\rangle$}




\title{Parser Evaluation across Frameworks without Format Conversion}
\author{Wai Lok Tam\\University of Tokyo\\7-3-1 Hongo Bunkyo-ku Tokyo
113-0033 Japan\\{\tt wailoktam\@yahoo.com}\And Yo Sato\\Dept of Computer Science\\Queen Mary University of London\\Mile End Road\\London E14NS, U.K.\\{\tt yo.sato@kcl.ac.uk}\And Yusuke Miyao \And Jun-ichi Tsujii}

\date{}

\begin{document}
\maketitle

\begin{abstract}
 
\end{abstract}




\section{Introduction}

%Although a computational grammar is usually part of a larger system, the name of the field, grammar engineering suggests that it is central to the system, at least from the point of views of grammar writers. This keeps grammar writers to focus on what they can do with a grammar when they want to look for a solution to a problem. To illustrate, grammar writers tend to rely more on inline comments than a separate manual for documenting a grammar. Such solution may not always be the best solution for a problem. If we are looking for documentation for grammar writers who have the need to understand how a grammar works and to edit the grammar, inline comments make a good solution.  But if we are looking for documentation for users, inline comments make very little sense to them, if they can access these comments at all. In this paper, we describe a solution for a different problem, the evaluation of a grammar. Our solution to this problem lies not in our grammars but in the manuals in which we provide the correct output for the linguistic phenomena covered by our two grammars. The idea is to match the output of a grammar against the sample output given in its manual. This means each grammar would get its own measuring stick. This is a major difference between our approach to evaluation and many other evaluation methods which make use of one measuring stick.

The traditional evaluation method for a deep parser is to test it against a list of sentences, each of which is paired with a yes or no. The parser is evaluated on the number of  grammatical sentences it accepts and that of ungrammatical sentences it rules out. A problem with this approach to evaluation is that it neither penalizes a parser for getting an analysis wrong for a sentence nor rewards it for getting it right. What prevents the NLP community from working out a universally applicable reward and penalty scheme is the absence of a gold standard that can be used across frameworks. The correctness of an analysis produced by a parser  can only be judged by matching it to the analysis produced by linguists in syntactic structures and semantic representations created specifically for the framework on which the grammar is based. A match or a mismatch between analyses produced by different parsers based on different frameworks does not lend itself for a meaningful comparison that leads to a fair evaluation of the parsers. To evaluate two parsers across frameworks, two kinds of methods suggest themselves:

\begin{enumerate}
\item Converting an analysis given in a certain format native to one framework to another native to a differernt framework (e.g. converting from a CCG \cite{steedman2000} derivation tree to an HPSG \cite{pollardandsag1994} phrase structure tree with AVM)
\item Converting analyses given in different framework-specific formats to some simpler format proposed as a framework-independent evaluation schema  (e.g. converting from HPSG phrase structure tree with AVM to GR \cite{briscodandcarrollandwatson2006})
\end{enumerate}

However, the feasibility of either solution is questionable. Even conversion between two evaluation schemata which make use of the simplest representation of syntactic information in the form of dependencies is reported to be problematic by \cite{miyaoandsagaeandtsujii2007}. 

In this paper, therefore, we propose a different method of parser evaluation that makes no attempt at any conversion of syntactic structures and semantic representations. We remove the need for such conversion by abstracting away from  comparison of syntactic structures and semantic representations. The basic idea is to generate a list of names of phenomena with each parse. These names of phenomena are matched against the phenomena given in the gold standard for the same sentence. The number of matches found is used for evaluating the parser that produces the parse.

\section{Research Problem}

Grammar formalisms differ in many aspects. In syntax, they differ in POS label assignment, phrase structure (if any), syntactic head assignment (if any) and so on, while in semantics, they differ from each other in semantic head assignment, role assignment, number of arguments taken by predicates, etc. Finding a common denominator between grammar formalisms in full and complex representation of syntactic information and semantic information has been generally considered by the NLP community to be an unrealistic task, although some serious attempts have been made recently to offer simpler representation of syntactic information \cite{briscodandcarrollandwatson2006,demarneffeandmaccartneyandmanning2006}. 
 
Briscoe et al \shortcite{briscodandcarrollandwatson2006}'s Grammatical Relation (GR) scheme is proposed as a framework-independent metric for parsing accuracy. The promise of GR lies actually in its dependence on a framework that makes use of simple representation of syntactic information. The assumption behind the usefulness of GR for evaluating the output of parsers %based on a grammar formalism other than Dependency Grammar 
is that most conflicts between grammar formalisms would be removed by discarding less useful information carried by complex syntactic or semantic representations used in grammar formalisms during conversion to GRs. But is this assumption true? The answer is not clear. A GR represents syntactic information in the form of  a binary relation between a token assigned as the head of the relation and other tokens assigned as its dependents. Notice however that grammar frameworks considerably disagree in the way they assign heads and non-heads. This would raise the doubt that, no matter how much information is removed, there could still remain disagreements between grammar formalisms in what is left.

The simplicity of GR, or other dependency-based metrics, may give the impression that conversion from a more complex representation into it is easier than conversion between two complex representations. In other words, GRs or a similar dependency relation looks like a promising candidate for \emph{lingua franca} of grammar frameworks. However the experiment results given by  Miyao et al \shortcite{miyaoandsagaeandtsujii2007} show that even conversion into GRs of predicate-argument structures, which is not much more complex than GRs, is not a trivial task.  Miyao et al \shortcite{miyaoandsagaeandtsujii2007} manage to convert 80\% of the predicate-argument structures outputted by their deep parser, ENJU, to GRs correctly. However the parser, with an over 90\% accuracy, is too good for the 80\% conversion rate. The lesson here is that simplicity of a representation is a different thing from simplicity in converting into that representation. 


\section{Outline of our Solution}

The problem of finding a  common denominator for grammar formalisms and the problem of conversion to a common denominator may be best addressed by evaluating parsers without making any attempt to find a common denominator or conduct any conversion. Let us describe briefly in this section how such evaluation can be realised.  


\subsection{Creating the Gold Standard} 
The first step of our evaluation method is to construct or find a number of sentences and get an annotator to mark each sentence for the phenomena illustrated by each sentence. After annotating all the sentences in a test suite, we get a list of pairs, whose first element is a sentence ID and second is again a list, one of the corresponding phenomena. This list of pairs is our gold standard. To illustrate, suppose we only get sentence \ref{johngivesaflowertomary} and sentence \ref{johngivesmaryaflower} in our test suite.

\enumsentence{John gives a flower to Mary}\label{johngivesaflowertomary}

\vspace{-7mm}

\enumsentence{John gives Mary a flower}\label{johngivesmaryaflower}

Sentence \ref{johngivesaflowertomary} is assigned the phenomena: proper noun, unshifted ditransitive, preposition. Sentence \ref{johngivesmaryaflower} is assigned the phenomena: proper noun, dative-shifted ditransitive. Our gold standard is thus the following list of pairs: 

\smallskip

\< \<\ref{johngivesaflowertomary}, $\langle{}${\smaller proper noun, unshifted ditransitive, preposition}\>,

\hspace{1.5mm} \<\ref{johngivesmaryaflower}, $\langle{}${\smaller proper noun,dative-shifted ditransitive}$\rangle$ 
$\rangle$



\subsection{Phenomena Recognition}


The second step of our evaluation method requires a small program that recognises what phenomena are illustrated by an input sentence taken from the test suite based on the output resulted from parsing the sentence. The recogniser provides a set of conditions that assign names of phenomena to an output, based on which the output is matched with some framework-specific regular expressions. It looks for hints like the rule being applied at a node, the POS label being assigned to a node, the phrase structure and the role assigned to a reference marker. The names of phenomena assigned to a sentence are stored in a list. The list of phenomena forms a pair with the ID of the sentence, and running the recogniser on multiple outputs obtained by batch parsing (with the parser to be evaluated) will produce a list of such pairs, in exactly the same format as our gold standard. Let us illustrate this with a parser that:


\begin{enumerate}
\item assigns a monotransitive verb analysis to `give' and an adjunct analysis to `to Mary' in \ref{johngivesaflowertomary}

\item assigns a ditransitive verb analysis to `give' in  \ref{johngivesmaryaflower} 
\end{enumerate}


The list of pairs we obtain from running the recogniser on the results produced by batch parsing the test suite with the parser to be evaluated is the following:

 \< \<\ref{johngivesaflowertomary}, \<proper noun, monotransitive, preposition, adjunct\>\>, 

\hspace{1.5mm}\<\ref{johngivesmaryaflower}, \<proper noun,dative-shifted ditransitive\> \>

\subsection{Performance Measure Calculation}

Comparing the two list of pairs generated from the previous steps, we can calculate the precision and recall of a parser using the following formulae:
\begin{align}
          Precision &= (\sum_{i=1}^n \frac{\mid\{R_{i}\}\cap\{A_{i}\}}{\mid\{R_{i}\}\mid})\div n
\end{align}

\begin{align}
          Recall &= (\sum_{i=1}^n \frac{\mid\{R_{i}\}\cap\{A_{i}\}}{\mid\{A_{i}\}\mid})\div n
\end{align}

\noindent where list $R_i$  is the list generated by the recognizer for sentence $i$, list $A_{i}$ is the list produced by annotators for sentence $i$, and $n$ the number of sentences in the test suite.

In our example, the parser that does a good job with dative-shifted ditransitives but does a poor job with unshifted ditranstives would have a precision of:

 \begin{align*}
            (\frac{2}{4} + \frac{2}{2})\div 2 &= 0.75
\end{align*}

\noindent and a recall of:

 \begin{align*}
            (\frac{2}{3} + \frac{2}{2})\div 2 &= 0.83
\end{align*}


\section{Refining our Solution}

\subsection{Exhaustive Annotation vs Non-exhaustive Annotation}
In order for the precision and recall given above to be a fair measure, it is necessary for both the recogniser and the annotators to produce an exhaustive list of the phenomena illustrated by a sentence. % 
%
%\begin{lstlisting}
%def recognize_or_annotate(NODE)
%    global(PHENOMENON_LIST,FLAG_LIST) 
%    FLAG_LIST.append(generate_satisfied_requirements_for\\
%_phenomena_that_involves_>1_constituents(NODE))
%    PHENOMENON_LIST.append(translate_pos_label_to_name_of\\
%_phenomena(NODE))%

%def expand_tree(MOTHER):
%    DAUGHTER_LIST = generate_daughters(MOTHER)
%    if len(DAUGHTER_LIST) = 1:
%        recognize_or_annotate(DAUGHTER_LIST[0])
%    else:
%#assume that non-binary tree is not allowed
%        recognize_or_annotate(DAUGHTER_LIST[0])
%        expand_tree(DAUGHTER_LIST[0])
%        recognize_or_annotate(DAUGHTER_LIST[1])
%        expand_tree(DAUGHTER_LIST[1])

%def find_phenomena_whose_requirements_r_\\
%satisfied(FLAG_LIST):
%    global(PHENOMENON_LIST)
%    for PHENOMENON in PREDEF_LIST_OF_PHENOMENA_THAT_INVOLVE_>1_CONSTITUENTS:
%        REQ_LIST = generate_requirements_for_phenomena_that_involves_>1_constituents(PHENOMENON)
%        for REQ in REQ_LIST:
%            SATISFIED = 0
%            for FLAG in FLAG_LIST:
%                if FLAG==REQ:
%                    SATISFIED=1
%        if SATISFIED == 1:
%            PHENOMENON_LIST.append(PHENOMENON)
%
%#begin main program
%while 1:
%    SENTENCE=TESTSUITE.readline()
%    if not SENTENCE: break
%    expand_tree(SENTENCE)
%    find_phenomena_whose_requirements_r_satisfied(FLAG_LIST)
    
%\end{lstlisting}

%Our recognizer(or a human annotator) is supposed to go through each node in a binary phrase structure tree from top to bottom until all the leafs are processed. The processing of each node is done by the procedure \textit{recognize\_or\_annotate}. This procedure calls two procedures: \textit{generate\_satisfied\_requirements\_for\_phenomena\_that\_involves\_>1\_constituents} and \textit{translate\_pos\_label\_to\_name\_of\_phenomena}. The names fo these procedures tell what they do. \textit{generate\_satisfied\_requirements\_for\_phenomena\_that\_involves\_>1\_constituents}  deals with phenomena that involves more than one constituents. An example of such phenomena is ''nominal gerund''. A possessive and a gerund that takes a noun phrase complement are required for forming a nominal gerund. Given a possessive, this procedure would mark one of the requirements for forming a nominal gerund as satisfied with a flag to be appended to a list of flags. The other procedure \textit{translate\_pos\_label\_to\_name\_of\_phenomena} deals with phenomena that involves only one constituent. The name of one such phenomenon is just what the POS label of a node stands for. For example, the POS label NNP would be translated into the phenomenon ''proper noun''. After processing all the leafs, we get a list of all the phenomena that involves only one consitutent and a list of flags that tell, among the phenomena that involve more than one constituents, which requirements for which of these phenomena are satisfied. The last thing that our recognizer (or a human annotator) has to do is to call \textit{find\_phenomena\_whose\_requirements\_r\_satisfied}. This procedure does what its name says, that is, finding out,  among the phenomena that involve more than one constituents, those phenomena whose requirements are all satisfied and append them to the list of phenomena that involves only one constituent. 
%
But we foresee that annotation errors are likely to be a problem of exhaustive annotation, as is reported in Miyao et al \shortcite{miyaoandsagaeandtsujii2007} for the gold standard described in Briscoe et al \shortcite{briscodandcarrollandwatson2006}. Exhaustive annotation procedures require annotators to repeatedly parse a sentence in search for a number of phenomena, which is not the way language is normally processed by humans. Forcing annotators to do this, particularly for a long and complex sentence, is a probable reason for the annotation errors in the gold standard described in \cite{briscodandcarrollandwatson2006}.

To avoid the same problem in our creation of a gold standard, we propose to allow non-exhaustive annotation. In fact, our proposal is to limit the number of phenomena assigned to a sentence to one. This decision on which phenomenon to be assigned is made, when the test suite is constructed, for each of the sentences contained in it. Following the traditional approach, we include every sentence in the test suite, along with the core phenomenon we intend to test it on \cite{lehmannandoepen1996}. %During the construction of the gold standard, the phenomenon assigned to every sentence is the core phenomenon we want to test with the sentence when we include it in the test suite. 
Thus, Sentence \ref{johngivesaflowertomary} would be assigned the phenomenon of unshifted ditransitive. Sentence \ref{johngivesmaryaflower} would be assigned the phenomenon of dative-shifted ditransitive. This revision of annotation policy removes the need for exhaustive annotation. Instead, annotators are given a new task. They are asked to assign to each sentence \emph{the most common error that a parser is likely to make}.  Thus Sentence \ref{johngivesaflowertomary} would be assigned adjunct for such an error. Sentence \ref{johngivesmaryaflower} would be assigned the error of noun-noun compound. Note that these errors are also names of phenomena. 

This change in annotation policy calls for a change in the calculation of precision and recall. We leave the recogniser as it is, i.e. to produce an exhaustive list of phenomena, since it is far beyond our remit to render it intelligent enough to select a single, intended, phenomenon. Therefore, an incorrectly low precision would result from a mismatch between the exhaustive list generated by the recogniser and the singleton list produced by annotators for a sentence. %$\mid\{phenomena\ in\ list\ R_{i}\}\cap\{phenomena\ in\ list\ A_{i}\}\mid$ would be 1 no matter how many phenomena illustrated by a sentence but not assigned to the sentence in the gold standard is generated by the recognizer. 
For example, suppose we only have sentence \ref{johngivesmaryaflower} in our test suite and the parser correctly analyses the sentence. Our recogniser assigns two phenomena (proper noun, dative-shifted ditransitive) to this sentence as before. This would result in a precision of 0.5. 

Thus we need to revise our definition of precision, but before we give our new definition, let us define a truth function $t$:

\[
t(A \supset B) = \left\{ 
\begin{array}{ccc}
1 & A & \supset B  \\
0 & A \cap B & =\emptyset 
\end{array} \right.
\]

\[
t(A \cap B=\emptyset) = \left\{ 
\begin{array}{ccc}
0 & A \cap B & \neq \emptyset \\
1 & A \cap B & =\emptyset 
\end{array} \right.
\]

Now, our new definition of precision and recall is as follows:

\begin{align}
        &  Precision  \\
       =& \frac{(\sum_{i=1}^n \frac{t(\{R_{i}\}\supset\{AP_{i}\})+t(\{R_{i}\}\cap\{AN_{i}\}=\emptyset)}{2})}{n} \notag
\end{align}\label{redefinedprecision}

\begin{align}
        & Recall \\
       =& \frac{(\sum_{i=1}^n \frac{\mid\{R_{i}\}\cap\{AP_{i}\}}{\mid\{AP_{i}\}\mid})}{n} \notag
\end{align}


\noindent where list $AP_{i}$ is the list of phenomena produced by annotators for sentence $i$, and list $AN_{i}$ is the list of errors produced by annotators for sentence $i$.

While the change in the definition of recall is trivial, the new definition of precision requires some explanation. The exhaustive list of phenomena generated by our recognizer for each sentence is taken as a combination of two answers to two questions on the two lists produced by annotators for each sentence. The correct answer to the question on the one-item-list of phenomenon produced by annotators for a sentence is a superset-subset relation between the list generated by our recognizer and the one-item-list of phenomenon produced by annotators. The correct answer to the question on the one-item-list of error produced by annotators for a sentence is the non-existence of any common member between the list generated by our recognizer and the one-item-list of error produced by annotators.

To illustrate, let us try a parser that does a good job with dative-shifted ditransitives but does a poor job with unshifted ditranstives on both  \ref{johngivesmaryaflower} and \ref{johngivesaflowertomary}. The precision of such a parser would be:

 \begin{align*}
            (\frac{0}{2} + \frac{2}{2})\div 2 &= 0.5
\end{align*}

and its recall would be:

\begin{align*}
            (\frac{0}{1} + \frac{1}{1})\div 2 &= 0.5
\end{align*}

\section{Experiment}

For this abstract, we evaluate ENJU \cite{miyao2006}, a released deep parser based on the HPSG formalism and a parser based on the Dynamic Syntax formalism \cite{kempsonandmeyerviolandgabbay2001} under development against the gold standard given in table \ref{experiment}.
%\begin{figure}
\begin{figure}
\begin{tabular}{| c | p{80pt} | p{80pt} |}
\hline
ID & Phenomenon & Error \\
\hline
\hline
1 &   unshifted ditransitive & adjunct \\
2 &   dative-shifted ditransitive & noun-noun compound \\
3 &   passive & adjunct \\
4 &  nominal gerund & verb that takes verbal complement \\
5 &  verbal gerund & imperative\\
6 &  preposition & particle \\
7 &  particle & preposition\\
8 & adjective with extrapolated sentential complement & relative clause\\
9 &  inversion & question \\
10 &  raising & control \\ 
\hline
\end{tabular}
\caption{Gold Standard for Parser Evaluation}\label{experiment}
\end{figure}
%\caption{}
%\end{figure}

\begin{figure}
\begin{tabular}{| c | c |}
\hline
ID & Sentence \\
\hline
\hline
1 &  John gives a flower to Mary \\
2 &  John give Mary a flower \\
3 &  John is dumped by Mary \\
4 & Your walking me pleases me \\
5 &  Abandoning children increased\\
6 & He talks to Mary \\
7 &  John makes up the story \\
8 & It is obvious that John is a fool \\
9 &  Hardly does anyone know Mary\\
10 &  John continues to please Mary \\ 
\hline
\end{tabular}
\caption{Sentences Used in the Gold Standard}\label{sentences}
\end{figure}

The precision and recall of the two parsers (ENJU and DSPD, which stands for ''Dynamic Syntax Parser under Development'')  are given in table \ref{result}:

\begin{figure}
\begin{tabular}{| c | c  | c |}
\hline
  Measure & ENJU & DSPD \\
\hline
  Precision & 0.8 & 0.7 \\
  Recall & 0.7 & 0.5 \\ 
\hline
\end{tabular}
\caption{Performance of Two Parsers}\label{result}
\end{figure}

The experiment that we report here is intended to be an experiment with the evaluation method described in the last section, rather than a very serious attempt to evaluate the two parsers in question. The sentences in table \ref{experiment} are carefully selected to include both sentences that illustrate core phenomena and sentences that illustrate rarer but more interesting (to linguists) phenomena. But there are too few of them. In fact, the most important number that we have obtained from our experiment is the 100\% success rate in recognizing the phenomena given in table \ref{experiment}.

\section{Discussion and Conclusion}

\subsection{Recognition Rate}
The 100\% success rate is not as surprising as it may look. We made use of two recognizers, one for each parser. Each of them is written by the one of us who is somehow involved in the development of the parser whose output is being recognized and familiar with the formalism on which the output is based. This is a clear advantage to format conversion used in other evaluation methods, which is usually done by someone familiar with either the source or the target of conversion, but not both. A recognizer only requires knowledge of one formalism and one parser. It is less demanding. For someone who is involved in the writing of the grammar for evaluation and the development of the parser that runs the grammar, the writing of the recognizer can make use of a lot of codes built in the parser or rules included in the grammar. It is essentially native to the parser to be evaluated whereas a converter is necessarily at least half foreign to a parser whose output is being converted. We can imagine that the 100\% recognition rate would drop a bit if we need to recognize a large number of sentences but are not allowed sufficient time to write very detailed regular expressions. Even in such a situation, we are confident that the success rate of recognition would be much higher than conversion under the same time constraint for programming. Note that the effectiveness of the evaluation method described in this paper depends on the success rate of recognition pretty much like the effectiveness of the evaluation method which make use of a format claimed to be framework independent (\cite{briscodandcarrollandwatson2006}, \cite{demarneffeandmaccartneyandmanning2006}) depends on the success rate of conversion to the proposed format. Given the high success rate of recognition, we argue that our evaluation method is more effective than any evaluation method which makes use of a format claimed to be framework independent and involves conversion of output based on a different formalism to the proposed format. 

\subsection{Strictness of Recognition and Precision}
There are some precautions regarding the use of our evaluation method. The redefined precision \ref{redefinedprecision} is affected by the strictness of the recognizer. To illustrate, let us take  sentence 8 in table\ref{experiment} as an example. ENJU basically gets the right rule and the right phrase structure for this sentence but makes some mistakes in assigning roles to the adjective and the copular verb. The recognizer we write for ENJU is very strict and refuses to assign the phenomenon ''adjective with extrapolated sentential complement'' based on the output given by ENJU. So ENJU gets 0 point for its answer to the question on the one-item-list of phenomenon in the gold standard. But it gets 1 point for its answer to  the question on the one-item-list of error in the gold standard because it does not go to the other extreme: a relative clause analysis, yielding a 0.5 precision. In this case, this value is fair for ENJU, which produces a partially correct analysis. However, a parser that does not accept the sentence, a parser that fails to produce output that can be recognized  or a parser that makes an unexpected mistake by analyzing sentence 8 as illustrating some other phenomenon would still get a 0.5 precision simply because its output does not show that it assigns a relative clause analysis to (some fragment of) sentence 8. We can easily deal with parsers that do not accept the sentence or fail to produce recognizable output by an exception clause which says that a parser that does not accept a sentence or its output cannot be recognized would get 0 precision (for that sentence). Parsers that make unexpected mistakes are problematic. An obvious solution to deal with these parsers is to come up with an exhaustive list of mistakes that can be made by parsers in the gold standard but that is even more unrealistic than creating an exhaustive list of phenomena illustrated by a sentence. We would leave the search for a solution to this problem as future work. For the moment, a temporary but realistic solution would be to expand the list of errors assigned to each sentence in the gold standard and ask annotators to make more intelligent guess of the mistakes that can be made by parsers by considering factors such as similarities in phrase structures or the sharing of sub-trees. 

\subsection{Combining Evaluation Methods}

For all measures, some distortion is unavoidable when applied to exceptional cases. This is true for the classic precision and recall. And our redefined precision and recall is no exception. In the case of the classic precision and recall, the distortion is countered by the inverse relation between them so that even if one score is distorted, we can tell from the other score that how well(poor) the object of evaluation performs. And our redefined precision and recall works pretty much the same way. What motives us to derive measures so closely related to the classic precision and recall used by other evaluation methods is the ease to combine the redefined precision and recall obtained from our evaluation method with the classic precision and recall obtained from other evaluation methods to obtain a full picture of the performance of the object of evaluation. For example, our redefined precision and recall figures given in table \ref{result} (or figures obtained from running the same experiment on a larger test set)  for ENJU can be combined with the precision and recall figures given in \cite{miyao2006} for ENJU based on a evaluation method that compares the predicate-argument structures outputted by ENJU to those given in Penn Treebank. The precision and recall figures given in \cite{miyao2006} is calculated by assigning equal weight to every sentence in section 23 of Penn Treebank. This means, phenomena-wise, different weights are assigned to different phenomena depending on the frequency a phenomenon occurs in section 23 of the Penn Treebank. Such assignment of weights may not be what linguists or developers of NLP systems for processing texts that have a very different distribution of phenomena from section 23 of the Penn Treebank want. For example, a linguist would be more interested in seeing equal weight being assigned across phenomena or even more weight being assigned to ''interesting'' phenomena. A developer of a question-answering system  would want precision and recall figures calculated with questions carrying the same, if not more weight than other phenomena which appear more often in section 23 of Penn Treebank but are of little use to him. In other words, phenomena-wise, the classic precision and recall figures calculated by assigning equal weight to every sentence in section 23 of Penn Treebank are skewed whereas frequency-wise, our redefined precision and recall figures are skewed. Less common phenomena are assigned weights disproportional to the frequency they occur in some or all domains. Our redefined precision and recall are not only useful for those who want a performance measure skewed the way they want, but also useful for those who want a performance measure unskewed, which can be obtained by combining our redefined precision and recall yielded from the evaluation method described in this paper with the equally skewed classic precision and recall yielded from other evaluation methods.

  



%The problem of finding a  common denominator for grammar formalisms and the problem of conversion to a common denominator may be best addressed by not finding a common denominator and not doing any conversion. For each grammar we intend to evaluate, we create a separate set of sample output in a format native to the grammar or a simplified format close-to-native to the grammar. The representations given as sample output are sorted by linguistic phenomena. Then we create two small programs, one for recognizing the linguistic phenomenon illustrated by an input sentence in a test-suite, which includes a equal number of sentences for each phenomenon covered by the grammar, another for  matching the output obtained from parsing the input sentence to the sample output given for the recognized phenomenon in the manual created for that grammar. These two programs are just sets of regular expressions. Evaluation of two grammars can be done by comparing the precision of the pasers loaded with them and the coverage of them. The precision of a parser loaded with a grammar for evaluation can be obtained by:



 


%\section{Problems with One Measuring Stick for All Formalism}

%Using one measuring stick for evaluation makes much more sense than using a different measuring stick for every grammar to be evaluated. But there are two difficult problems with using a single measuring stick for evaluating more than one grammars. 




%For each grammar we intend to evaluate, we create a separate set of sample output in a format native to the grammar or a simplified format close-to-native to the grammar. The representations given as sample output are sorted by linguistic phenomena. Then we create two small programs, one for recognizing the linguistic phenomenon illustrated by an input sentence in a test-suite, which includes a equal number of sentences for each phenomenon covered by the grammar, another for  matching the output obtained from parsing the input sentence to the sample output given for the recognized phenomenon in the manual created for that grammar. These two programs are just sets of regular expressions.

%\subsection{Implementation}

%We have tried this evaluation method with the grammar of ENJU \cite{miyao2005}, a released deep parser based on HPSG \cite{pollardandsag1994} and a grammar under development based on Dynamic Syntax \cite{kempsonandgabbayandmeyerviol2001}.  
\bibliographystyle{coling}

\bibliography{proposal}{}



\end{document}

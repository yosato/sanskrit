\documentclass{article}
\usepackage{coling}

\usepackage[CJKspace]{XeCJK}
\setCJKfamilyfont{zhrm}{SharpSong.ttf}
\setCJKfamilyfont{jarm}{AozoraMinchoRegular.ttf}

\title{Classification and adaptation of Japanese dialects: an unsupervised }

\begin{document}

\maketitle

\begin{abstract}
  
\end{abstract}

\section{Introduction}

In what follows we propose a pipeline through which to derive a classified set of language models for dialects, given a body of `mixed' corpus composed of different dialects as yet unclassified, when one of them however is already sufficiently resourced. The test case we experiment on is Japanese, for which the `standard' dialect, the Tokyo variety, is sufficiently resourced, i.e. equipped with its lexicon and PoS tagged corpora. Taking this as the starting point, our task is to derive dialect language models, and classify the corpus into dialect sub-portions. The pipeline will consist of three main stages. At the itinial stage we divide the corpus simply into two sub-parts, fully resourced and not fully resourced portions, on the basis of the language model for the fully resourced dialect. This will then allow for the second stage, where the technique of `cognate pair' detection is used to spot the potential equivalents in the resourced dialect to the unknown portions the less-resourced one. At the third stage, we agglomeratively cluster these unknown parts using the 

OOV On the newly found vocabulary of these equivalents, we conduct an initial clustering, to arrive at a tentative subgrouping of texts. Finally an N-gram language model is then created for each subgroup, and a new loop of clustering is applied until a convergence is reached. The net results are the classification of dialects each equipped with adapted lexicon and language model.

\section{Related work}

Our overall scheme is generally inspired by the series of work relating to the concept of cognates  \cite{Scherrer12}, though there are several important departures. One is that  text processing   cognitive aspects.


 and a new classification asis, and continues until   and build the    the classification is conducted by clustering, with the N-gram , where the corpus is sub-divided into dialects. We then have a second phase where the language , to derive the classified language model for each dialect, based on   one of which is already sufficiently `resourced,' the   and the language model of \emph{one} of these dialects, they are classified   a language model for each, in our case for Japanese.  and the lexicon of \emph{one} of these dialects.  OOV segmented corpus of  A word, or more precisely a word type, can be pronounced in various ways. In real life, the same sequence of phonemes are realised in a wide range of sounds, depending on various factors such as localib
ties, registers and individual habits, and this is sometimes reflected in their written forms. An emphasised and elongated `so good' may be written `soooo goooood' or a Cockney or French-accented `hello' may be written `'allo'. These variants essentially constitute the same word type, but to the  occasio and registers. This could in fact 

\section{}




\cite{SaitoEtAl14}  a supervised method of


\section{Method overview}

The overall procedure of our proposal consists in three stages. First, we run a procedure for detecting out-of-vocabulary (OOV) words. frequent unknowns.

\section{Conclusion}

\end{document}

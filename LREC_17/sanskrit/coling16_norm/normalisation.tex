\documentclass[11pt]{article}
\usepackage{coling2016}
\usepackage{tipa}
\usepackage{times}
\usepackage{latexsym}

%\usepackage[CJKspace]{XeCJK}
%\setCJKfamilyfont{zhrm}{SharpSong.ttf}
%\setCJKfamilyfont{jarm}{AozoraMinchoRegular.ttf}

\title{Detecting orthographical variants: a normalisation method combining phones and contexts}

\begin{document}

\maketitle

\begin{abstract}
This paper presents...

\end{abstract}

\section{Introduction}

A word, or more precisely, a word type, can be pronounced in various ways. In real life, the same sequence of phonemes are realised in a wide range of sounds, depending on various factors such as localities, registers and individual habits, and this is sometimes reflected in their written forms. An emphasised and elongated `so good' may be written `soooo goooood' or a Cockney or French-accented `hello' may be written `'allo'. The opposing pairs essentially constitute the same word type, and whichever version is used, the conversing interlocuters would normally not be hindered in their comprehension by such variations whereas, for a computer, all these variants represent different entities, unless some normalisation mechanism tells it otherwise. 

In what follows we propose a simple method of normalising such 'out-of-vocabulary' (OOV) items in a manner that is plausible, that is, close to what humans do, taking as target four languages: Japanese, Korean and Vietnamese. This emphasis on plausibility is at the centre of our proposal: we model how people actually do. This does not mean however that we do not attach due importance to the usefulness as a tool, but we try to be general and render it applicable to any orthography. This is where we differ from the methods that have already been proposed and implemented. More specifically our proposal is different in one or more of the following three ways. One is that it does not presuppose word boundaries, and hence, it is applicable to the orthographies where no word boundary is explicitly marked. Secondly, our procedure relies primarily on sounds, more specifically on the unit of \emph{syllables}, which we consider to be the very element that enables the reader to decipher a variant straightforwardly. Notice, incidentally, that the choice of the above four languages is motivated by these two factors, as they are equipped with syllable-based, not word-based, orthographies. 

Lastly but not the least, our normalisation procedure itself is unsupervised, while we assume a language model. We therefore model the typical situation where a person with the full vocabulary of a language is faced with a sentence, part of which is `not exactly' what s/he knows but is rather easily identifiable. Imagine that a Cockney person talks to you and says `Aw are you mite': even without a prior experience with such an accent, you will likely realise what s/he said. So our learner takes as the starting point the OOV words left out by its already acquired knowledge, and derive their equivalent on the basis of phonetic and contextual similarities, without any explicit mapping. %In short, we simulate a situation that constantly faces a human listener: a person who is already equipped with the knowledge of a language, but is faced with continuous stream of phonemes, where there may be variants of the words s/he already knows.


\section{Related work}

A number of efforts to normalise orthographically `noisy' corpora have been published since a large body of informal corpora have been made publicly available, particularly Twitter, e.g. for orthographies with spaces between words (`spaced' orthgraphy, let's say)  \cite{HanBaldwin11} and for  \cite{Saito14}  for ones without (`glued' orthography). 

On the other hand sound similarity measures   speech recognition


\section{Method Overview}

As we stated earlier, our method primarily relies on syllables, so the data is first `syllabified'. That is, all the texts are syllable-segmented so that each segment corresponds one-to-one to a syllable, simulating `the sound unit' segmentation\footnote{}. This can be done relatively straightforwardly for the four languages in question, as their orthography is more or less syllable-oriented. As a matter of fact almost nothing needs to be done for Korean and Vietnamese, since every character represents a syllable in the former, while in the latter syllables are demarcated by spaces. For Korean however, where post-positional phrases and verb projections are customarily segmented by spaces, we remove the spaces as well, since our goal is to be able to handle the texts with no word boundary. The situation is slightly more complicated for Japanese, as \emph{kanji}s, a set of ideographic characters, do not meet the condition of one-to-one correspondence to syllables. This situation can however be remedied by using \emph{kana}, a syllable-based, phonemic orthography. Therefore for Japanese, the texts are pre-processed so that all the texts be rendered kanas.

Notice that this means that as texts are made deliberately less readable than they are in Korean and Japanese, making the task harder than as it actually is. This is due to the aforementioned main objective of our work, which is to model the mechanism of human agent, faced with a continuous syllable stream. However there is an important additional factor that facilitates the human agent, and that is \emph{prosody}. Though it is impossible to capture all the prosodic aspects on the text level, we use what is available: punctuations and, for Vietnames, tones, and for Japanese, lexical accents.   

The second phase of the procedure is to parse the texts with a trained language model to perform word segmentation. The purpose of this stage is as much to find known words as to identify `unknown' parts in the text. This is done with the help of the tried and tested statistical sequential labelling technique. We use MeCab, a tool developed by \cite{KudoEtAl04}, with the option of enabling `unknown handling' with a configuration where any string could be an OOV item of any category. For Japanese, since the syllabified (kana-rendered) texts diviate from the standard form, the lexicon is kana-rendered too and the model is modified accordingly. For Vietnamese and Korean there is no need for such manipulations. While such unknown parts could be of any length, and consequently, of any number of words as well, this does not matter too much, since the following procedure will ensure that all such items that are dissimilar to an existent word will be ignored. 

We essentially obtain, through the processing thus far, a partially parsed sentence with gaps composed of syllables. 

[\textipa{av}] are you [\textipa{}]

\noindent and the task is to find the standard in-volcabulary equivalents, `how' and `mate'.


We then focus on the `gaps' which consist of syllables, and use two types of similarities to find the equivalents in the known lexicon. First, a phonemic similarity measure finds probable replacement candidates. Second, for each such candidate, the context similarity is computed, using a skip-gram word embedding technique. The details of these similarity checks will be described in the following subsections. We then combine the two measures in the optimal way, and evaluate the results in two manners. Firstly we test the accuracy against the hand-annotated gold standard. Secondly we gauge the efficacy as a language model by perplexity. On both counts, we also The results show that despite the imposed `handicap' (syllabification) for generality, our procedure performs on par with 

\section{Unknown string detection}

The second phase of the procedure is to parse the texts with a trained language model to perform word segmentation. The purpose of this stage is as much to find known words as to identify `unknown' parts in the text. This is done with the help of the tried and tested statistical sequential labelling technique. We use MeCab, a tool developed by \cite{KudoEtAl04}, with the option of enabling `unknown handling' with a configuration where any string could be an OOV item of any category. For Japanese, since the syllabified (kana-rendered) texts diviate from the standard form, the lexicon is kana-rendered too and the model is modified accordingly. For Vietnamese and Korean there is no need for such manipulations. While such unknown parts could be of any length, and consequently, of any number of words as well, this does not matter too much, since the following procedure will ensure that all such items that are dissimilar to an existent word will be ignored. 

We essentially obtain, through the processing thus far, a partially parsed sentence with gaps composed of syllables. 

[\textipa{av}] are you [\textipa{}]

\noindent and the task is to find the standard in-volcabulary equivalents, `how' and `mate'.

\section{Similarity measures}

Our model is unsupervised, i.e. there is no hand-crafted model mappings as in \cite{SaitoEtAl14}. Therefore the canonical equivalent to an OOV item needs to be found in the dataset itself, and we do this by the similarities inherent in the data. The method employed here is somewhat similar to that of Han and Baldwin \cite{HanBaldwin11}, in that it invokes two different types of criteria, contexts and surface forms, and uses one to narrow down (`shortlist') the candidates and the other for the final selection. We opt however, conversely to Han and Baldwin, to use the surface criteria for shortlisting and the context for the final decision. 

The reasons, which will be discussed in more detail below, are roughly as follows. First, we do not use any supervised method in this process, so the context-finding part could not pinpoint as precisely as in Han and Baldwin, who use dependency graphs. Second, to our advantage, this stage follows a preceding stage of unknown string detection,   

\subsection{Syllable phonemic similarity}

The first, shortlisting, phase consists of verifying if there are words in the canonical lexicon that sound similar to the unknown strings. We check each unknown string against the canonical lexicon and see if there is a word sounding similar. To do this we use a string-distance metric,  \cite{}


By means of this metric, we compute the similarity, for each pair of an unknown string, against words in the canonical lexicon. On paper, therefore, the potential complexity is the number of unknown strings multiplied by that of lexicon entries, which could be a huge number. Practically however the latter number can be restricted by setting the threshold for the character count difference and also for the similarity metric itself. 



\subsection{Context similarity}



\section{Evaluation}

\section{Results}

\section{Conclusion}

\end{document}
